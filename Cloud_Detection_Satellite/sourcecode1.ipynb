{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow with GPU",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/priyanshgupta1998/Image_Processing/blob/master/Cloud_Detection_Satellite/sourcecode1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1nBNcC_agkM",
        "colab_type": "text"
      },
      "source": [
        "#Understanding Clouds from Satellite Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xpc_e0lhaCJY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKfZeg1VaCMA",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "061d8d9f-8b04-4444-de1c-0798a7ae75f0"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c8312188-b6b0-4c73-9553-a30fe2dedeaf\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-c8312188-b6b0-4c73-9553-a30fe2dedeaf\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mu-JzepDaCN0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "f33b91eb-eb5e-4b1d-9209-22adeee6a7c2"
      },
      "source": [
        "!kaggle competitions download -c understanding_cloud_organization"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading train_images.zip to /content\n",
            "100% 3.44G/3.44G [00:45<00:00, 59.2MB/s]\n",
            "100% 3.44G/3.44G [00:45<00:00, 81.6MB/s]\n",
            "Downloading test_images.zip to /content\n",
            "100% 2.29G/2.30G [00:39<00:00, 70.3MB/s]\n",
            "100% 2.30G/2.30G [00:39<00:00, 62.0MB/s]\n",
            "Downloading train.csv.zip to /content\n",
            " 96% 52.0M/54.2M [00:00<00:00, 44.8MB/s]\n",
            "100% 54.2M/54.2M [00:01<00:00, 55.9MB/s]\n",
            "Downloading sample_submission.csv to /content\n",
            "  0% 0.00/321k [00:00<?, ?B/s]\n",
            "100% 321k/321k [00:00<00:00, 103MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXA9xrdyaCRf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install catalyst\n",
        "!pip install pretrainedmodels\n",
        "!pip install git+https://github.com/qubvel/segmentation_models.pytorch\n",
        "!pip install pytorch_toolbelt\n",
        "!pip install torchvision==0.4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nw2pgAafaCcp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import collections\n",
        "import time \n",
        "import tqdm\n",
        "from PIL import Image\n",
        "from functools import partial\n",
        "train_on_gpu = True\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
        "\n",
        "import albumentations as albu\n",
        "from albumentations import torch as AT\n",
        "\n",
        "from catalyst.data import Augmentor\n",
        "from catalyst.dl import utils\n",
        "from catalyst.data.reader import ImageReader, ScalarReader, ReaderCompose, LambdaReader\n",
        "from catalyst.dl.runner import SupervisedRunner\n",
        "from catalyst.contrib.models.segmentation import Unet\n",
        "from catalyst.dl.callbacks import DiceCallback, EarlyStoppingCallback, InferCallback, CheckpointCallback\n",
        "\n",
        "import segmentation_models_pytorch as smp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHisP5sWaCpS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_img(x, folder: str='train_images'):\n",
        "    \"\"\"\n",
        "    Return image based on image name and folder.\n",
        "    \"\"\"\n",
        "    data_folder = f\"{path}/{folder}\"\n",
        "    image_path = os.path.join(data_folder, x)\n",
        "    img = cv2.imread(image_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    return img\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j90KDFEXaCwI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rle_decode(mask_rle: str = '', shape: tuple = (1400, 2100)):\n",
        "    '''\n",
        "    Decode rle encoded mask.\n",
        "    \n",
        "    :param mask_rle: run-length as string formatted (start length)\n",
        "    :param shape: (height, width) of array to return \n",
        "    Returns numpy array, 1 - mask, 0 - background\n",
        "    '''\n",
        "    s = mask_rle.split()\n",
        "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
        "    starts -= 1\n",
        "    ends = starts + lengths\n",
        "    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
        "    for lo, hi in zip(starts, ends):\n",
        "        img[lo:hi] = 1\n",
        "    return img.reshape(shape, order='F')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPO5TWRAaC6x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_mask(df: pd.DataFrame, image_name: str='img.jpg', shape: tuple = (1400, 2100)):\n",
        "    \"\"\"\n",
        "    Create mask based on df, image name and shape.\n",
        "    \"\"\"\n",
        "    encoded_masks = df.loc[df['im_id'] == image_name, 'EncodedPixels']\n",
        "    masks = np.zeros((shape[0], shape[1], 4), dtype=np.float32)\n",
        "\n",
        "    for idx, label in enumerate(encoded_masks.values):\n",
        "        if label is not np.nan:\n",
        "            mask = rle_decode(label)\n",
        "            masks[:, :, idx] = mask\n",
        "            \n",
        "    return masks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCg4MRmwht0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_tensor(x, **kwargs):\n",
        "    \"\"\"\n",
        "    Convert image or mask.\n",
        "    \"\"\"\n",
        "    return x.transpose(2, 0, 1).astype('float32')\n",
        "\n",
        "\n",
        "def mask2rle(img):\n",
        "    '''\n",
        "    Convert mask to rle.\n",
        "    img: numpy array, 1 - mask, 0 - background\n",
        "    Returns run length as string formated\n",
        "    '''\n",
        "    pixels= img.T.flatten()\n",
        "    pixels = np.concatenate([[0], pixels, [0]])\n",
        "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
        "    runs[1::2] -= runs[::2]\n",
        "    return ' '.join(str(x) for x in runs)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8544-thhtwB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualize(image, mask, original_image=None, original_mask=None):\n",
        "    \"\"\"\n",
        "    Plot image and masks.\n",
        "    If two pairs of images and masks are passes, show both.\n",
        "    \"\"\"\n",
        "    fontsize = 14\n",
        "    class_dict = {0: 'Fish', 1: 'Flower', 2: 'Gravel', 3: 'Sugar'}\n",
        "    \n",
        "    if original_image is None and original_mask is None:\n",
        "        f, ax = plt.subplots(1, 5, figsize=(24, 24))\n",
        "\n",
        "        ax[0].imshow(image)\n",
        "        for i in range(4):\n",
        "            ax[i + 1].imshow(mask[:, :, i])\n",
        "            ax[i + 1].set_title(f'Mask {class_dict[i]}', fontsize=fontsize)\n",
        "    else:\n",
        "        f, ax = plt.subplots(2, 5, figsize=(24, 12))\n",
        "\n",
        "        ax[0, 0].imshow(original_image)\n",
        "        ax[0, 0].set_title('Original image', fontsize=fontsize)\n",
        "               \n",
        "        for i in range(4):\n",
        "            ax[0, i + 1].imshow(original_mask[:, :, i])\n",
        "            ax[0, i + 1].set_title(f'Original mask {class_dict[i]}', fontsize=fontsize)\n",
        "        \n",
        "        ax[1, 0].imshow(image)\n",
        "        ax[1, 0].set_title('Transformed image', fontsize=fontsize)\n",
        "        \n",
        "        \n",
        "        for i in range(4):\n",
        "            ax[1, i + 1].imshow(mask[:, :, i])\n",
        "            ax[1, i + 1].set_title(f'Transformed mask {class_dict[i]}', fontsize=fontsize)\n",
        "            \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MsktwQ2iIMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "            \n",
        "def visualize_with_raw(image, mask, original_image=None, original_mask=None, raw_image=None, raw_mask=None):\n",
        "    \"\"\"\n",
        "    Plot image and masks.\n",
        "    If two pairs of images and masks are passes, show both.\n",
        "    \"\"\"\n",
        "    fontsize = 14\n",
        "    class_dict = {0: 'Fish', 1: 'Flower', 2: 'Gravel', 3: 'Sugar'}\n",
        "\n",
        "    f, ax = plt.subplots(3, 5, figsize=(24, 12))\n",
        "\n",
        "    ax[0, 0].imshow(original_image)\n",
        "    ax[0, 0].set_title('Original image', fontsize=fontsize)\n",
        "\n",
        "    for i in range(4):\n",
        "        ax[0, i + 1].imshow(original_mask[:, :, i])\n",
        "        ax[0, i + 1].set_title(f'Original mask {class_dict[i]}', fontsize=fontsize)\n",
        "\n",
        "\n",
        "    ax[1, 0].imshow(raw_image)\n",
        "    ax[1, 0].set_title('Original image', fontsize=fontsize)\n",
        "\n",
        "    for i in range(4):\n",
        "        ax[1, i + 1].imshow(raw_mask[:, :, i])\n",
        "        ax[1, i + 1].set_title(f'Raw predicted mask {class_dict[i]}', fontsize=fontsize)\n",
        "        \n",
        "    ax[2, 0].imshow(image)\n",
        "    ax[2, 0].set_title('Transformed image', fontsize=fontsize)\n",
        "\n",
        "\n",
        "    for i in range(4):\n",
        "        ax[2, i + 1].imshow(mask[:, :, i])\n",
        "        ax[2, i + 1].set_title(f'Predicted mask with processing {class_dict[i]}', fontsize=fontsize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlDXRYB-iIIr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_with_augmentation(image, mask, augment):\n",
        "    \"\"\"\n",
        "    Wrapper for `visualize` function.\n",
        "    \"\"\"\n",
        "    augmented = augment(image=image, mask=mask)\n",
        "    image_flipped = augmented['image']\n",
        "    mask_flipped = augmented['mask']\n",
        "    visualize(image_flipped, mask_flipped, original_image=image, original_mask=mask)\n",
        "    \n",
        "    \n",
        "sigmoid = lambda x: 1 / (1 + np.exp(-x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUpZXjZMiNzm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def post_process(probability, threshold, min_size):\n",
        "    \"\"\"\n",
        "    Post processing of each predicted mask, components with lesser number of pixels\n",
        "    than `min_size` are ignored\n",
        "    \"\"\"\n",
        "    # don't remember where I saw it\n",
        "    mask = cv2.threshold(probability, threshold, 1, cv2.THRESH_BINARY)[1]\n",
        "    num_component, component = cv2.connectedComponents(mask.astype(np.uint8))\n",
        "    predictions = np.zeros((350, 525), np.float32)\n",
        "    num = 0\n",
        "    for c in range(1, num_component):\n",
        "        p = (component == c)\n",
        "        if p.sum() > min_size:\n",
        "            predictions[p] = 1\n",
        "            num += 1\n",
        "    return predictions, num"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDgqrrdciNqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_training_augmentation():\n",
        "    train_transform = [\n",
        "\n",
        "        albu.HorizontalFlip(p=0.5),\n",
        "        albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=0.5, border_mode=0),\n",
        "        albu.GridDistortion(p=0.5),\n",
        "        albu.OpticalDistortion(p=0.5, distort_limit=2, shift_limit=0.5),\n",
        "        albu.Resize(320, 640)\n",
        "    ]\n",
        "    return albu.Compose(train_transform)\n",
        "\n",
        "\n",
        "def get_validation_augmentation():\n",
        "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
        "    test_transform = [\n",
        "        albu.Resize(320, 640)\n",
        "    ]\n",
        "    return albu.Compose(test_transform)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLus43dviNnM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_preprocessing(preprocessing_fn):\n",
        "    \"\"\"Construct preprocessing transform\n",
        "    \n",
        "    Args:\n",
        "        preprocessing_fn (callbale): data normalization function \n",
        "            (can be specific for each pretrained neural network)\n",
        "    Return:\n",
        "        transform: albumentations.Compose\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    _transform = [\n",
        "        albu.Lambda(image=preprocessing_fn),\n",
        "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
        "    ]\n",
        "    return albu.Compose(_transform)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQMkZFKpiS3k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dice(img1, img2):\n",
        "    img1 = np.asarray(img1).astype(np.bool)\n",
        "    img2 = np.asarray(img2).astype(np.bool)\n",
        "\n",
        "    intersection = np.logical_and(img1, img2)\n",
        "\n",
        "    return 2. * intersection.sum() / (img1.sum() + img2.sum())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaTm2QciiSx5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeRQNWnPhuO8",
        "colab_type": "text"
      },
      "source": [
        "We have folders with train and test images, file with train image ids and masks and sample submission."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt8J28TWhtor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = '../input/understanding_cloud_organization'\n",
        "os.listdir(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHnVaszegkkd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv(f'{path}/train.csv')\n",
        "sub = pd.read_csv(f'{path}/sample_submission.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0o9Q0jhUhkwG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iR3neT_3hkqK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_train = len(os.listdir(f'{path}/train_images'))\n",
        "n_test = len(os.listdir(f'{path}/test_images'))\n",
        "print(f'There are {n_train} images in train dataset')\n",
        "print(f'There are {n_test} images in test dataset')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esnW9lVBhKZL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train['Image_Label'].apply(lambda x: x.split('_')[1]).value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FulibY3rhf9_",
        "colab_type": "text"
      },
      "source": [
        "So we have ~5.5k images in train dataset and they can have up to 4 masks: Fish, Flower, Gravel and Sugar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkA3d5XqhKUT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.loc[train['EncodedPixels'].isnull() == False, 'Image_Label'].apply(lambda x: x.split('_')[1]).value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ylb6fGGhKPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.loc[train['EncodedPixels'].isnull() == False, 'Image_Label'].apply(lambda x: x.split('_')[0]).value_counts().value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp0trUrdhYUL",
        "colab_type": "text"
      },
      "source": [
        "But there are a lot of empty masks. In fact only 266 images have all four masks. It is important to remember this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWkihkv6hKNM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train['label'] = train['Image_Label'].apply(lambda x: x.split('_')[1])\n",
        "train['im_id'] = train['Image_Label'].apply(lambda x: x.split('_')[0])\n",
        "\n",
        "\n",
        "sub['label'] = sub['Image_Label'].apply(lambda x: x.split('_')[1])\n",
        "sub['im_id'] = sub['Image_Label'].apply(lambda x: x.split('_')[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h83CLYFJhKKy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize=(25, 16))\n",
        "for j, im_id in enumerate(np.random.choice(train['im_id'].unique(), 4)):\n",
        "    for i, (idx, row) in enumerate(train.loc[train['im_id'] == im_id].iterrows()):\n",
        "        ax = fig.add_subplot(5, 4, j * 4 + i + 1, xticks=[], yticks=[])\n",
        "        im = Image.open(f\"{path}/train_images/{row['Image_Label'].split('_')[0]}\")\n",
        "        plt.imshow(im)\n",
        "        mask_rle = row['EncodedPixels']\n",
        "        try: # label might not be there!\n",
        "            mask = rle_decode(mask_rle)\n",
        "        except:\n",
        "            mask = np.zeros((1400, 2100))\n",
        "        plt.imshow(mask, alpha=0.5, cmap='gray')\n",
        "        ax.set_title(f\"Image: {row['Image_Label'].split('_')[0]}. Label: {row['label']}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxs0HvNehQw0",
        "colab_type": "text"
      },
      "source": [
        "We can see that masks can overlap. Also we can see that clouds are really similar to fish, flower and so on. Another important point: masks are often quite big and can have seemingly empty areas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DfMvkWIhMKS",
        "colab_type": "text"
      },
      "source": [
        "#Preparing data for modelling\n",
        "At first, let's create a list of unique image ids and the count of masks for images. This will allow us to make a stratified split based on this count."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8D-I4k5gkjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "id_mask_count = train.loc[train['EncodedPixels'].isnull() == False, 'Image_Label'].apply(lambda x: x.split('_')[0]).value_counts().\\\n",
        "reset_index().rename(columns={'index': 'img_id', 'Image_Label': 'count'})\n",
        "train_ids, valid_ids = train_test_split(id_mask_count['img_id'].values, random_state=42, stratify=id_mask_count['count'], test_size=0.1)\n",
        "test_ids = sub['Image_Label'].apply(lambda x: x.split('_')[0]).drop_duplicates().values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MPVYLMZhF3m",
        "colab_type": "text"
      },
      "source": [
        "#Exploring augmentations with albumentations\n",
        "One of important things while working with images is choosing good augmentations. There are a lot of them, let's have a look at augmentations from albumentations!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymaKuM9egkhr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_name = '8242ba0.jpg'\n",
        "image = get_img(image_name)\n",
        "mask = make_mask(train, image_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1u4bJsdFgkgC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "visualize(image, mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pz90spXbgxsG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_with_augmentation(image, mask, albu.HorizontalFlip(p=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwJnFujhgxnf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_with_augmentation(image, mask, albu.VerticalFlip(p=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wx0bAULBgkeV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_with_augmentation(image, mask, albu.RandomRotate90(p=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKY2mZPTgkcc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_with_augmentation(image, mask, albu.ElasticTransform(p=1, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKVmY3pRgkY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_with_augmentation(image, mask, albu.GridDistortion(p=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1I-jj4haC40",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_with_augmentation(image, mask, albu.OpticalDistortion(p=1, distort_limit=2, shift_limit=0.5))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgzCko7hgXSi",
        "colab_type": "text"
      },
      "source": [
        "#Setting up data for training in Catalyst"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_Y5UHVVgJM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class CloudDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame = None, datatype: str = 'train', img_ids: np.array = None,\n",
        "                 transforms = albu.Compose([albu.HorizontalFlip(),AT.ToTensor()]),\n",
        "                preprocessing=None):\n",
        "        self.df = df\n",
        "        if datatype != 'test':\n",
        "            self.data_folder = f\"{path}/train_images\"\n",
        "        else:\n",
        "            self.data_folder = f\"{path}/test_images\"\n",
        "        self.img_ids = img_ids\n",
        "        self.transforms = transforms\n",
        "        self.preprocessing = preprocessing\n",
        "    def __getitem__(self, idx):\n",
        "        image_name = self.img_ids[idx]\n",
        "        mask = make_mask(self.df, image_name)\n",
        "        image_path = os.path.join(self.data_folder, image_name)\n",
        "        img = cv2.imread(image_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        augmented = self.transforms(image=img, mask=mask)\n",
        "        img = augmented['image']\n",
        "        mask = augmented['mask']\n",
        "        if self.preprocessing:\n",
        "            preprocessed = self.preprocessing(image=img, mask=mask)\n",
        "            img = preprocessed['image']\n",
        "            mask = preprocessed['mask']\n",
        "        return img, mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_ids)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiaDU5zIgJLE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ENCODER = 'resnet50'\n",
        "ENCODER_WEIGHTS = 'imagenet'\n",
        "DEVICE = 'cuda'\n",
        "\n",
        "ACTIVATION = None\n",
        "model = smp.Unet(\n",
        "    encoder_name=ENCODER, \n",
        "    encoder_weights=ENCODER_WEIGHTS, \n",
        "    classes=4, \n",
        "    activation=ACTIVATION,\n",
        ")\n",
        "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDX5Y0gZgJIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_workers = 0\n",
        "bs = 16\n",
        "train_dataset = CloudDataset(df=train, datatype='train', img_ids=train_ids, transforms = get_training_augmentation(), preprocessing=get_preprocessing(preprocessing_fn))\n",
        "valid_dataset = CloudDataset(df=train, datatype='valid', img_ids=valid_ids, transforms = get_validation_augmentation(), preprocessing=get_preprocessing(preprocessing_fn))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True, num_workers=num_workers)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=bs, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "loaders = {\n",
        "    \"train\": train_loader,\n",
        "    \"valid\": valid_loader\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvmKNJBqgJCR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 19\n",
        "logdir = \"./logs/segmentation\"\n",
        "\n",
        "# model, criterion, optimizer\n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': model.decoder.parameters(), 'lr': 1e-2}, \n",
        "    {'params': model.encoder.parameters(), 'lr': 1e-3},  \n",
        "])\n",
        "scheduler = ReduceLROnPlateau(optimizer, factor=0.15, patience=2)\n",
        "criterion = smp.utils.losses.BCEDiceLoss(eps=1.)\n",
        "runner = SupervisedRunner()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRxDx-pYgJAG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "runner.train(\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    loaders=loaders,\n",
        "    callbacks=[DiceCallback(), EarlyStoppingCallback(patience=5, min_delta=0.001)],\n",
        "    logdir=logdir,\n",
        "    num_epochs=num_epochs,\n",
        "    verbose=True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5rH7vCRaC3R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "utils.plot_metrics(\n",
        "    logdir=logdir, \n",
        "    # specify which metrics we want to plot\n",
        "    metrics=[\"loss\", \"dice\", 'lr', '_base/lr']\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BE99ujj-aC10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoded_pixels = []\n",
        "loaders = {\"infer\": valid_loader}\n",
        "runner.infer(\n",
        "    model=model,\n",
        "    loaders=loaders,\n",
        "    callbacks=[\n",
        "        CheckpointCallback(\n",
        "            resume=f\"{logdir}/checkpoints/best.pth\"),\n",
        "        InferCallback()\n",
        "    ],\n",
        ")\n",
        "valid_masks = []\n",
        "probabilities = np.zeros((2220, 350, 525))\n",
        "for i, (batch, output) in enumerate(tqdm.tqdm(zip(\n",
        "        valid_dataset, runner.callbacks[0].predictions[\"logits\"]))):\n",
        "    image, mask = batch\n",
        "    for m in mask:\n",
        "        if m.shape != (350, 525):\n",
        "            m = cv2.resize(m, dsize=(525, 350), interpolation=cv2.INTER_LINEAR)\n",
        "        valid_masks.append(m)\n",
        "\n",
        "    for j, probability in enumerate(output):\n",
        "        if probability.shape != (350, 525):\n",
        "            probability = cv2.resize(probability, dsize=(525, 350), interpolation=cv2.INTER_LINEAR)\n",
        "        probabilities[i * 4 + j, :, :] = probability"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htXDxaQYf86Q",
        "colab_type": "text"
      },
      "source": [
        "#Find optimal values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWtoA0lraC0F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_params = {}\n",
        "for class_id in range(4):\n",
        "    print(class_id)\n",
        "    attempts = []\n",
        "    for t in range(0, 100, 5):\n",
        "        t /= 100\n",
        "        for ms in [0, 100, 1200, 5000, 10000]:\n",
        "            masks = []\n",
        "            for i in range(class_id, len(probabilities), 4):\n",
        "                probability = probabilities[i]\n",
        "                predict, num_predict = post_process(sigmoid(probability), t, ms)\n",
        "                masks.append(predict)\n",
        "\n",
        "            d = []\n",
        "            for i, j in zip(masks, valid_masks[class_id::4]):\n",
        "                if (i.sum() == 0) & (j.sum() == 0):\n",
        "                    d.append(1)\n",
        "                else:\n",
        "                    d.append(dice(i, j))\n",
        "            attempts.append((t, ms, np.mean(d)))\n",
        "\n",
        "    attempts_df = pd.DataFrame(attempts, columns=['threshold', 'size', 'dice'])\n",
        "\n",
        "\n",
        "    attempts_df = attempts_df.sort_values('dice', ascending=False)\n",
        "    print(attempts_df.head())\n",
        "    best_threshold = attempts_df['threshold'].values[0]\n",
        "    best_size = attempts_df['size'].values[0]\n",
        "    \n",
        "    class_params[class_id] = (best_threshold, best_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnP2z8mdaCss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(class_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSmGmPmXaClc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.lineplot(x='threshold', y='dice', hue='size', data=attempts_df);\n",
        "plt.title('Threshold and min size vs dice for one of the classes');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIexQdpsaCkG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, (input, output) in enumerate(zip(\n",
        "        valid_dataset, runner.callbacks[0].predictions[\"logits\"])):\n",
        "    image, mask = input\n",
        "        \n",
        "    image_vis = image.transpose(1, 2, 0)\n",
        "    mask = mask.astype('uint8').transpose(1, 2, 0)\n",
        "    pr_mask = np.zeros((350, 525, 4))\n",
        "    for j in range(4):\n",
        "        probability = cv2.resize(output.transpose(1, 2, 0)[:, :, j], dsize=(525, 350), interpolation=cv2.INTER_LINEAR)\n",
        "        pr_mask[:, :, j], _ = post_process(sigmoid(probability), class_params[j][0], class_params[j][1])\n",
        "    #pr_mask = (sigmoid(output) > best_threshold).astype('uint8').transpose(1, 2, 0)\n",
        "    \n",
        "        \n",
        "    visualize_with_raw(image=image_vis, mask=pr_mask, original_image=image_vis, original_mask=mask, raw_image=image_vis, raw_mask=output.transpose(1, 2, 0))\n",
        "    \n",
        "    if i >= 2:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abq1gNW4fpAn",
        "colab_type": "text"
      },
      "source": [
        "#Predicting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpQxLDEyaCiE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGx9sWUwaCf9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dataset = CloudDataset(df=sub, datatype='test', img_ids=test_ids, transforms = get_validation_augmentation(), preprocessing=get_preprocessing(preprocessing_fn))\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
        "\n",
        "loaders = {\"test\": test_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWRZ8lWoaCak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoded_pixels = []\n",
        "image_id = 0\n",
        "for i, test_batch in enumerate(tqdm.tqdm(loaders['test'])):\n",
        "    runner_out = runner.predict_batch({\"features\": test_batch[0].cuda()})['logits']\n",
        "    for i, batch in enumerate(runner_out):\n",
        "        for probability in batch:\n",
        "            \n",
        "            probability = probability.cpu().detach().numpy()\n",
        "            if probability.shape != (350, 525):\n",
        "                probability = cv2.resize(probability, dsize=(525, 350), interpolation=cv2.INTER_LINEAR)\n",
        "            predict, num_predict = post_process(sigmoid(probability), class_params[image_id % 4][0], class_params[image_id % 4][1])\n",
        "            if num_predict == 0:\n",
        "                encoded_pixels.append('')\n",
        "            else:\n",
        "                r = mask2rle(predict)\n",
        "                encoded_pixels.append(r)\n",
        "            image_id += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vuJKUM4aCYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub['EncodedPixels'] = encoded_pixels\n",
        "sub.to_csv('submission.csv', columns=['Image_Label', 'EncodedPixels'], index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnSHhx1-aCWT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}